# -*- coding: utf-8 -*-
"""ETL and Analysis on Online Retail Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u7t-VvFrwSiORfNWdsBVGaIZvwRi-BPk

# **Detailed Data Engineering Project with Spark: ETL and Analysis on Online Retail Data**

Hello data enthusiasts! In this blog post, we will learn how to perform an ETL process and analysis on e-commerce data using PySpark and Spark. We will cover steps like cleaning, transforming, and visualizing the data. Using a real-world dataset called the Online Retail Data Set, we will analyze e-commerce metrics such as customer types, sales trends, and the best-selling products.
MEHMETCAN ANGÜN

## **Steps of the Project**

✤ Data Loading and Exploration

✤ Data Cleaning and Filtering

✤ Feature Engineering and Data Transformation

✤ Data Analysis and Visualization

# **Required Tools and Setup**

In this project, we will use the following tools and libraries:

✤ PySpark

✤ Pandas

✤ Matplotlib

✤ Databricks Community Edition

# **Data Loading and Exploration**
Let’s first load the libraries we will use in our project:
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, countDistinct, lit, when, avg
import pandas as pd
import matplotlib.pyplot as plt

"""⇝ SparkSession: Allows us to create a session for Spark operations.


⇝ pyspark.sql.functions: Contains the necessary functions for working with Spark DataFrames. We will use functions like column access (col), date format conversion (to_date), counting distinct values (countDistinct), conditional assignment (when), etc.


⇝ pandas and matplotlib.pyplot: Pandas will be used to convert PySpark DataFrames to traditional Python DataFrames, and Matplotlib will allow us to create visualizations.

# **Let’s start by initializing a Spark session:**


"""

spark = SparkSession.builder.appName("AdvancedOnlineRetailETL").getOrCreate()

"""This line creates a session for Spark operations. The appName parameter is used to give the session a descriptive name. You can choose any name you like.

Now, let’s load the data. For those using Databricks like me, you can easily upload your data in the Databricks environment by navigating to Catalog > dbfs > fileStore > tables in the left menu and selecting the upload button to get the path:


"""

df = spark.read.csv("Online_Retail.csv", header=True, inferSchema=True)

"""The csv() function reads the CSV file.

⇝ header=True: Defines the first row of the CSV file as column names.

⇝ inferSchema=True: Automatically infers data types (e.g., numerical or date values are correctly loaded).

Let’s check the schema of the loaded data with .printSchema() and view the first 5 rows with df.show(5).



Since the InvoiceNo column serves as a unique identifier for each sale, it may appear as an integer (int) at first glance. However, in many datasets like this one, InvoiceNo can contain alphanumeric characters (both numbers and letters), especially in cases where certain letters are added to invoices of canceled orders (e.g., "C12345"). Therefore, I left the column as a string type (or StringType in Spark). If you are sure that the dataset contains only numbers, you can change the format with the following code:


"""

df = df.withColumn("InvoiceNo", col("InvoiceNo").cast("integer"))

"""# **Handling Missing Data and Basic Filtering**

E-commerce data often contains faulty or missing records. Therefore, we will remove rows with missing values in columns like InvoiceNo, StockCode, Description, Quantity, UnitPrice, and CustomerID. We will also convert the Quantity and UnitPrice columns to appropriate data types:


"""

df_cleaned = df.dropna(subset=['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'UnitPrice', 'CustomerID'])
df_cleaned = df_cleaned.withColumn("Quantity", col("Quantity").cast("integer"))
df_cleaned = df_cleaned.withColumn("UnitPrice", col("UnitPrice").cast("double"))
df_cleaned = df_cleaned.withColumn("TotalValue", col("Quantity") * col("UnitPrice"))

"""✧ dropna(): This function removes rows with missing data in specific columns.

✧ withColumn(): Adds or updates a column in the DataFrame.

✧ The Quantity and UnitPrice columns are converted to integer and double types, respectively, so we can perform numerical operations.

✧ TotalValue: A new column is added to calculate the total value of each order, which is derived by multiplying Quantity (quantity) by UnitPrice (unit price). This gives us the monetary value of each order.

# **Feature Engineering and Data Transformation**

Now, let’s transform the date column format:
"""

df_cleaned = df_cleaned.withColumn("InvoiceDate", to_date(col("InvoiceDate"), "MM/dd/yyyy"))

"""☸ to_date(): Converts the InvoiceDate column into the MM/dd/yyyy format. This ensures compatibility when performing date-based analysis and reduces error risks.

# **Calculating Customer Shopping Frequency**

Customer shopping frequency is an important metric for sales optimization. We will see this more clearly in the visualization section. Let’s start with the necessary steps:



"""

customer_frequency = df_cleaned.groupBy("CustomerID").agg(countDistinct("InvoiceNo").alias("PurchaseCount"))
df_cleaned = df_cleaned.join(customer_frequency, on="CustomerID", how="left")

"""☸ groupBy("CustomerID"): Groups the data by the CustomerID column, so rows with the same CustomerID are grouped together.

agg(countDistinct("InvoiceNo").alias("PurchaseCount")):

☸ countDistinct("InvoiceNo"): Counts the distinct InvoiceNo values within each group, which indicates the number of purchases made by each customer.

☸ alias("PurchaseCount"): Assigns a name to the resulting column, which shows the shopping frequency.

join(customer_frequency, on="CustomerID", how="left"):

☸ join(): Joins two DataFrames.

☸ on="CustomerID": Uses CustomerID as the common column to join the two DataFrames.

☸ how="left": Performs a left join, keeping all rows from the df_cleaned table and adding the PurchaseCount values from the customer_frequency table. If no match is found in customer_frequency, the PurchaseCount will be null.

# **Identifying Customer Types (B2B and B2C)**

We will categorize customers based on their shopping frequency:


"""

df_cleaned = df_cleaned.drop(customer_frequency.PurchaseCount)
df_cleaned = df_cleaned.withColumn( "CustomerType", when(col("PurchaseCount") > 1, lit("B2B")).otherwise(lit("B2C")) )

"""➾ when(): Creates a conditional expression. If PurchaseCount > 1, the customer is labeled as B2B (business); otherwise, they are B2C (individual).

# **➾ lit(): Adds fixed values, such as "B2B" and "B2C".**

#** We can now view the results:**

➾ select(): Selects only the columns CustomerID, PurchaseCount, CustomerType, and InvoiceDate.

➾ distinct(): Retrieves unique rows.

➾ show(10): Displays the first 10 rows, showing each customer's type, shopping frequency, and last purchase date.

# **Our output will be like this:**
"""

distinct_customers = df_cleaned.select("CustomerID", "PurchaseCount", "CustomerType", "InvoiceDate").distinct()
distinct_customers.show(10)

"""# **Data Analysis and Visualization**

**Let’s start by analyzing the top-selling products:**

➳ groupBy("Description"): Groups the data by product description.

➳ sum("Quantity"): Calculates the total quantity sold for each product.

➳ orderBy(col("sum(Quantity)").desc()): Sorts by the total sales quantity in descending order, so the best-selling products appear first.

➳ show(10): Displays the top 10 best-selling products.

# **Let’s examine our output:**
"""

top_products = df_cleaned.groupBy("Description").sum("Quantity").orderBy(col("sum(Quantity)").desc())
top_products.show(10)

sales_trend = df_cleaned.groupBy("InvoiceDate").sum("Quantity").orderBy("sum(Quantity)", ascending=False)
sales_trend.show(10)

"""orderBy("sum(Quantity)", ascending=False): Sorts by total sales in descending order, with the most sales occurring first.

# **Let’s check our output:**

**We can also explore sales trends annually and monthly:**
"""

from pyspark.sql.functions import year, month

df_cleaned = df_cleaned.withColumn("Year", year(col("InvoiceDate")))
df_cleaned = df_cleaned.withColumn("Month", month(col("InvoiceDate")))

"""➳ year(col("InvoiceDate")): Extracts the year from the InvoiceDate column and adds it as a new Year column.

➳ month(col("InvoiceDate")): Extracts the month from the InvoiceDate column and adds it as a new Month column.


"""

monthly_sales = df_cleaned.groupBy("Year", "Month").sum("Quantity").orderBy("Year", "Month")
monthly_sales.show(24)

"""➣ groupBy("Year", "Month"): Groups the data by year and month, so sales within the same period are aggregated.

➣ orderBy("Year", "Month"): Sorts by year and month in ascending order to get a chronological sequence.

# **Now let’s look at our output:**

"""

df_cleaned = df_cleaned.withColumn("TotalValue", col("Quantity") * col("UnitPrice"))

"""➣ withColumn("TotalValue", ...): Creates a new column to calculate the total value of each order (Quantity × UnitPrice).

Finally, we group by CustomerID and sort by Avg Order Value:

➢ groupBy("CustomerID"): Groups the data by customer.

➢ agg(): Aggregates the data to calculate the average total value.

➢ round(avg("TotalValue"), 2).alias("Avg Order Value"): Calculates the average order value for each customer and rounds it to two decimal places.

➢ orderBy(col("Avg Order Value").desc()): Sorts by average order value in descending order.

## **Let’s check our output:**
"""

from pyspark.sql.functions import round as spark_round # Import Spark's round function and alias it

avg_order_value = df_cleaned.groupBy("CustomerID").agg(spark_round(avg("TotalValue"), 2).alias("Avg Order Value")).orderBy(col("Avg Order Value").desc())
avg_order_value.show()

"""top_products_pd = top_products.limit(10).toPandas():

↠ top_products: This is likely a Spark DataFrame that contains information about products and their sales.

↠ .limit(10): This method selects only the first 10 rows (the top 10 best-selling products).

↠ .toPandas(): This method converts the Spark DataFrame into a Pandas DataFrame. Pandas is a Python library commonly used for data manipulation and analysis. This conversion allows for easier manipulation and visualization of the data.

↠ plt.figure(figsize=(12, 6)):This function, from the matplotlib.pyplot library, sets the size of the plot. figsize=(12, 6) specifies that the width of the plot is 12 units and the height is 6 units.

plt.barh(top_products_pd['Description'],top_products_pd['sum(Quantity)'], color='skyblue'):

↠ plt.barh(): This function creates a horizontal bar chart.

↠ top_products_pd['Description']: This column contains the names or descriptions of the products, which are displayed on the y-axis of the chart.

↠ top_products_pd['sum(Quantity)']: This column contains the total sales quantity for each product (e.g., total units sold). This determines the length of each bar.

↠ color='skyblue': Sets the color of the bars to light blue.

↠ plt.xlabel("Total Sales"): This line adds the label “Total Sales” to the x-axis.

↠ plt.ylabel("Products"): This line adds the label “Products” to the y-axis.

↠ plt.title("Top 10 Best Selling Product"): This line sets the title of the chart to “Top 10 Best Selling Product,” which describes the data being visualized.

↠ plt.gca().invert_yaxis(): This line inverts the y-axis order. By default, the y-axis is ordered from bottom to top, but this command places the best-selling product at the top. It improves the readability of the chart.

↠ plt.show(): This command displays the plot. It makes the chart visible on the screen.

##**Let’s check our output:**

"""

top_products_pd = top_products.limit(10).toPandas()

plt.figure(figsize=(12, 6))
plt.barh(top_products_pd['Description'], top_products_pd['sum(Quantity)'], color='skyblue')
plt.xlabel("Total Sales")
plt.ylabel("Products")
plt.title("Top 10 Best Selling Product")
plt.gca().invert_yaxis()
plt.show()

"""⊛ marker='o': This option adds circular markers at each data point on the line.

⊛ linestyle='-': This specifies that the line connecting the data points will be solid.

⊛ color='blue': This sets the color of the line to blue.

⊛ plt.grid(True):This line adds a grid to the plot, making it easier to read and interpret the data points along the axes.

# **Let’s check our output:**
"""

monthly_sales_pd = monthly_sales.toPandas()

plt.figure(figsize=(14, 6))
plt.plot(monthly_sales_pd['Month'], monthly_sales_pd['sum(Quantity)'], marker='o', linestyle='-', color='blue')
plt.xlabel("Month")
plt.ylabel("Total Sales")
plt.title("Monthly Sales Trend")
plt.grid(True)
plt.show()

"""Finally, let’s examine the sales distribution by day of the week. First, let’s write our code:"""

customer_type_counts = df_cleaned.groupBy("CustomerType").count().toPandas()

plt.figure(figsize=(8, 6))
plt.pie(customer_type_counts['count'], labels=customer_type_counts['CustomerType'], autopct='%1.1f%%', colors=['lightcoral', 'skyblue'])
plt.title("Sales Distribution by Customer Types (B2B vs B2C)")
plt.show()

"""from pyspark.sql.functions import dayofweek:

◍ This imports the dayofweek function from PySpark's SQL functions module. The dayofweek function extracts the day of the week from a date or timestamp column, where Sunday is 1, Monday is 2, and so on, until Saturday, which is 7.

##**Let’s examine our final output:**

"""

from pyspark.sql.functions import dayofweek

df_cleaned = df_cleaned.withColumn("DayOfWeek", dayofweek(col("InvoiceDate")))
weekly_sales = df_cleaned.groupBy("DayOfWeek").sum("Quantity").orderBy("DayOfWeek").toPandas()

plt.figure(figsize=(10, 6))
plt.bar(weekly_sales['DayOfWeek'], weekly_sales['sum(Quantity)'], color='orange')
plt.xlabel("Day of the Week")
plt.ylabel("Total Sales")
plt.title("Sales distribution by day of the week")
plt.show()

"""# **Conclusion**

In this article, we thoroughly explored the data analysis process using the “Online Retail” dataset. After data cleaning and preparation, we generated key features like shopping frequency and customer types. We also analyzed metrics such as top-selling products, sales trends, and weekly sales distributions. The visualizations provided valuable insights, helping e-commerce businesses optimize their sales strategies.

As we continue advancing in data engineering and analytics, working with large datasets and deriving meaningful insights becomes increasingly important. This project provides a solid foundation for those who want to learn how to analyze and visualize e-commerce data, and it serves as a starting point for more advanced studies and models.

Your feedback is highly valuable to me! Thanks for reading my project, and I wish you all the best in your work!

"""





